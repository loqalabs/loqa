services:
  # NATS Message Bus
  nats:
    image: nats:2.10-alpine
    container_name: nats
    ports:
      - "4222:4222"
      - "8222:8222"
    command: ["nats-server", "-js", "-m", "8222"]
    networks:
      - loqa-network
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 4222"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Ollama LLM Service
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - loqa-network
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Pull model on startup
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "ollama serve &
       sleep 10 &&
       ollama pull llama3.2:3b &&
       wait"

  # Speech-to-Text Service (OpenAI-compatible API)
  stt:
    image: fedirz/faster-whisper-server:latest-cpu
    container_name: stt
    ports:
      - "8000:8000"
    environment:
      - MODEL=tiny
      - DEVICE=cpu
    volumes:
      - stt-cache:/app/.cache/huggingface
    networks:
      - loqa-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Text-to-Speech Service (Kokoro-82M GPU)
  tts-gpu:
    image: ghcr.io/remsky/kokoro-fastapi-gpu:latest
    container_name: tts-gpu
    ports:
      - "8880:8880"
    environment:
      - CUDA_VISIBLE_DEVICES=0  # Use first GPU if available
    volumes:
      - tts-cache:/app/cache
    networks:
      - loqa-network
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/v1/audio/voices"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # TTS models take longer to load
    profiles:
      - gpu  # Only start with GPU profile

  # CPU-only TTS service (default for 5-minute setup)
  tts:
    image: ghcr.io/remsky/kokoro-fastapi-cpu:latest
    container_name: tts
    ports:
      - "8880:8880"
    volumes:
      - tts-cache:/app/cache
    networks:
      - loqa-network
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8880/v1/audio/voices"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s
    # No profile - runs by default for 5-minute setup compatibility

  # Loqa Hub - Central orchestrator
  hub:
    image: loqalabs/loqa-hub:latest
    container_name: hub
    ports:
      - "3000:3000"
      - "50051:50051"
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    restart: unless-stopped
    environment:
      - STT_URL=http://stt:8000
      - TTS_URL=http://tts:8880/v1
      - TTS_VOICE=af_bella
      - TTS_SPEED=1.0
      - TTS_FORMAT=mp3
      - TTS_NORMALIZE=true
      - TTS_MAX_CONCURRENT=10
      - TTS_TIMEOUT=10s
      - TTS_FALLBACK_ENABLED=true
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2:3b
      - NATS_URL=nats://nats:4222
      - NATS_SUBJECT=loqa.commands
      - LOQA_HOST=0.0.0.0
      - LOQA_PORT=3000
      - LOQA_GRPC_PORT=50051
      - LOG_LEVEL=info
      - LOG_FORMAT=json
    depends_on:
      nats:
        condition: service_healthy
      ollama:
        condition: service_healthy
      stt:
        condition: service_healthy
      tts:
        condition: service_healthy
    volumes:
      - hub-data:/app/data
    networks:
      - loqa-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Commander Administrative Dashboard
  commander:
    image: loqalabs/loqa-commander:latest
    container_name: commander
    ports:
      - "5173:80"
    environment:
      - VITE_HUB_API_URL=http://localhost:3000
    depends_on:
      hub:
        condition: service_healthy
    networks:
      - loqa-network

  # Test Relay (for gRPC connection testing only - no audio access in container)
  # For real voice testing, run the test relay directly on the host
  test-relay:
    image: loqalabs/loqa-mock-relay:latest
    container_name: test-relay
    environment:
      - HUB_ADDRESS=hub:50051
      - RELAY_ID=docker-mock-relay
      - WAKE_WORD_THRESHOLD=0.7
      - AUDIO_ENABLED=false  # Disable audio in container
    depends_on:
      hub:
        condition: service_healthy
    networks:
      - loqa-network
    # Note: This container cannot access host audio devices
    # Use for gRPC connection testing only
    profiles:
      - testing

volumes:
  ollama-data:
    external: true
    name: ${COMPOSE_PROJECT_NAME}_ollama-data
  hub-data:
    external: true
    name: ${COMPOSE_PROJECT_NAME}_hub-data
  stt-cache:
    external: true
    name: ${COMPOSE_PROJECT_NAME}_stt-cache
  tts-cache:
    external: true
    name: ${COMPOSE_PROJECT_NAME}_tts-cache

networks:
  loqa-network:
    driver: bridge