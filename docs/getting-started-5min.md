# ğŸš€ Get Loqa Running in 5 Minutes

The fastest way to experience Loqa's local-first voice assistant capabilities, including the real-time voice command timeline interface.

## âœ… Prerequisites (30 seconds)

```bash
# Verify you have Docker and Docker Compose
docker --version && docker-compose --version
```

**Requirements:**
- Docker & Docker Compose
- curl

## ğŸƒâ€â™‚ï¸ Complete System Setup (4 minutes)

**Just copy and paste this single command:**

```bash
curl -fsSL "https://raw.githubusercontent.com/loqalabs/loqa/main/setup.sh?$(date +%s)" | bash
```

That's it! This command:
- Downloads the setup script
- Downloads the Docker Compose configuration  
- Pulls all pre-built images
- Starts all services automatically

### Alternative: Full Repository
If you prefer to have the full repository locally:

```bash
git clone https://github.com/loqalabs/loqa.git && cd loqa && ./setup.sh
```

This automatically:
- ğŸ³ Starts NATS, Ollama, Hub, Device services, and Timeline UI
- ğŸ¤– Downloads Llama 3.2 3B model
- ğŸ“ Sets up Whisper.cpp for speech recognition
- ğŸ’¡ Configures simulated smart devices
- ğŸ“Š Launches web interface at http://localhost:5173

## ğŸ¤ Test Voice Commands (2-3 minutes)

The containerized test puck cannot access your microphone. For real voice testing, you need to run the test puck on your host system:

### Option 1: Quick Container Test (gRPC connection only)
```bash
# Start the test puck container (no audio, just connection testing)
docker-compose --profile testing up -d test-puck

# Check connection status
docker logs loqa-test-puck
```

### Option 2: Real Voice Testing (Recommended)
Use the automated voice testing script:

```bash
curl -fsSL "https://raw.githubusercontent.com/loqalabs/loqa/main/test-voice.sh?$(date +%s)" | bash
```

The script will:
- Check for Go and Git
- Clone the loqa-puck repository
- Install dependencies
- Start the test puck with microphone access
- Show you example voice commands to try

**Manual approach** (if you prefer):
```bash
# Clone the test puck repository
git clone https://github.com/loqalabs/loqa-puck.git
cd loqa-puck/test-go

# Install Go dependencies
go mod download

# Run the test puck with microphone access
go run ./cmd -hub localhost:50051

# Now speak these commands near your microphone:
# "Hey Loqa, turn on the kitchen lights"
# "Hey Loqa, play music in the living room"  
# "Hey Loqa, turn off all lights"
```

**Expected behavior:**
1. ğŸ¤ See "Voice detected!" in the terminal when you speak
2. ğŸ“ Watch speech-to-text conversion in real-time
3. ğŸ¤– See LLM parse your intent and extract commands
4. ğŸ’¡ Observe device actions in the service logs
5. ğŸ“Š **Watch events appear instantly in the Timeline UI at http://localhost:5173**

## ğŸ” Verify It's Working (1 minute)

**Check All Services:**
```bash
# Check all services are running (including Timeline UI)
docker-compose ps

# Watch the logs
docker-compose logs -f hub device-service observer
```

**Check Timeline UI:**
1. Open http://localhost:5173 in your browser
2. You should see the "Voice Command Timeline" interface
3. After speaking commands, events will appear showing:
   - ğŸ“ **Transcription** of what you said
   - ğŸ¯ **Intent** parsed by the AI (e.g., "turn_on_lights") 
   - ğŸ“Š **Confidence** score for the recognition
   - âœ…/âŒ **Success/failure** status
   - ğŸ”Š **Audio playback** of your original voice command
   - ğŸ“‹ **Full event data** when you click for details

**Manual Testing:**
```bash
# Test manual device command (will also appear in timeline)
nats pub loqa.devices.commands.lights '{
  "device_type": "lights",
  "action": "on", 
  "location": "bedroom"
}' --server=nats://localhost:4222
```

## ğŸ¯ What You Just Experienced

**Complete Voice-to-Visualization Pipeline:**
- ğŸ—£ï¸ **Voice Input** â†’ Puck captures audio via microphone
- ğŸ“¡ **gRPC Streaming** â†’ Audio sent to Hub service  
- ğŸ“ **Speech-to-Text** â†’ Whisper.cpp converts to text
- ğŸ§  **Intent Parsing** â†’ Ollama LLM extracts commands
- ğŸ’¾ **Event Storage** â†’ Hub records structured event data in SQLite
- ğŸ“¨ **Message Routing** â†’ NATS delivers to device service
- ğŸ  **Device Control** â†’ Smart home devices respond
- ğŸ“Š **Real-time UI** â†’ Timeline interface shows all events via API

**All Local & Private:**
- âœ… No cloud services involved
- âœ… No data leaves your network
- âœ… Full offline functionality
- âœ… Sub-2-second response times
- âœ… Complete observability of all voice interactions

## ğŸ› ï¸ Next Steps

### For Users
- **Timeline UI**: Explore dark mode, event filtering, and analytics
- **Add Real Devices**: Configure Home Assistant integration
- **Create Custom Skills**: Extend with your own voice commands

### For Developers
- ğŸ› ï¸ **[Developer Guide](./DEVELOPER.md)** - Development setup, testing, and contributing
- ğŸ—ï¸ **[Architecture Deep Dive](./architecture.md)** - System design and technical details
- ğŸ§© **[Build Your First Skill](./skills.md)** - Skills development guide

### Learn More
- ğŸ“– [Full Documentation](./quickstart.md)
- ğŸ”§ [Configuration Guide](./config.md)

## ğŸ›‘ Stop Everything

```bash
# Clean shutdown
docker-compose down
```

---

## â“ Troubleshooting

**Voice not detected?**
- Check microphone permissions
- Verify PortAudio installation: `brew install portaudio`
- Speak clearly and wait for "ğŸ¤ Voice detected!" message

**Docker issues?**
- Ensure Docker has sufficient memory (8GB+ recommended)
- Check port conflicts: `lsof -i :4222,3000,50051,11434`

**LLM responses slow?**
- First run downloads 2GB model (one-time)
- Ensure sufficient CPU/RAM (4+ cores, 8GB+ RAM recommended)
- Consider using smaller model: `OLLAMA_MODEL=llama3.2:1b`

---

## ğŸ“± What You've Built

You now have a complete local-first voice assistant system running:

**ğŸ¤ Voice Processing Stack:**
- Puck client using your laptop's microphone/speakers
- Hub service processing speech-to-text and intent parsing
- Device service controlling smart home devices (simulated)
- Timeline UI automatically running at http://localhost:5173

**ğŸ“Š Complete Observability:**
- Real-time web interface showing all voice interactions
- Event details with transcriptions, intents, and confidence scores
- Audio playback and full JSON event inspection
- Dark mode support and auto-refresh

**ğŸ”’ Privacy-First:**
- Everything runs on your local network
- No data sent to external services
- Complete control and ownership of your voice data

**ğŸ‰ Congratulations!** You're now running a complete local-first voice assistant that rivals commercial solutions â€” but respects your privacy.

*Average setup time: 4-5 minutes on modern hardware*