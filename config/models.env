# Centralized Model Configuration for Loqa Platform
# This file defines all AI model selections to ensure consistency across environments

# =============================================================================
# LLM CONFIGURATION
# =============================================================================

# Primary LLM Model (used for command parsing and intent recognition)
OLLAMA_MODEL=llama3.2:3b

# Streaming LLM Model (used for real-time response generation)
STREAMING_MODEL=llama3.2:3b

# =============================================================================
# STT CONFIGURATION
# =============================================================================

# STT Model (Whisper-based speech recognition)
# Options: tiny, base, small, medium, large
# Models with .en suffix are English-only and often more accurate for English
WHISPER_MODEL=base.en

# =============================================================================
# MODEL ENVIRONMENT PROFILES
# =============================================================================

# Development Profile (prioritizes accuracy for testing)
# - LLM: llama3.2:3b (good balance of accuracy and speed)
# - STT: base.en (accurate English recognition)

# Production Profile (prioritizes performance for deployment)
# - LLM: llama3.2:1b (faster inference, lower resource usage)
# - STT: base.en (maintains accuracy with reasonable performance)

# =============================================================================
# CONFIGURATION NOTES
# =============================================================================

# LLM Model Trade-offs:
# - llama3.2:1b  - Fastest, lowest memory usage, good for simple commands
# - llama3.2:3b  - Balanced performance and understanding, recommended default
# - llama3.2:8b  - Best understanding, higher resource requirements

# STT Model Trade-offs:
# - tiny     - Fastest, lower accuracy
# - base     - Good balance for multilingual
# - base.en  - Good balance, English-only, often more accurate for English
# - small    - Better accuracy, moderate performance impact
# - medium   - High accuracy, significant performance impact
# - large    - Highest accuracy, substantial resource requirements

# =============================================================================
# USAGE INSTRUCTIONS
# =============================================================================

# To use these centralized configurations:
# 1. Source this file in docker-compose: `env_file: ./config/models.env`
# 2. Import in scripts: `source ./config/models.env`
# 3. Reference in Dockerfiles: `ENV OLLAMA_MODEL=${OLLAMA_MODEL}`
# 4. Use in service configurations to maintain consistency

# To change models system-wide:
# 1. Update values in this file
# 2. Restart affected services
# 3. Verify model availability in service logs